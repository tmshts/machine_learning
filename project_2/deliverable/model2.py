# -*- coding: utf-8 -*-
"""model2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aWGNL8ofd6yW995TRgzfkJMpM6yeL15F
"""

import os
import urllib.request as http
from zipfile import ZipFile

import tensorflow as tf
import numpy as np
from PIL import Image
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import save_model, load_model
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorflow.keras import utils
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, Dense
from tensorflow.keras import Sequential
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping


def load_cifar10(num_classes=3):
    """
    Downloads CIFAR-10 dataset, which already contains a training and test set,
    and return the first `num_classes` classes.
    Example of usage:

    >>> (x_train, y_train), (x_test, y_test) = load_cifar10()

    :param num_classes: int, default is 3 as required by the assignment.
    :return: the filtered data.
    """
    (x_train_all, y_train_all), (x_test_all, y_test_all) = cifar10.load_data()

    fil_train = tf.where(y_train_all[:, 0] < num_classes)[:, 0]
    fil_test = tf.where(y_test_all[:, 0] < num_classes)[:, 0]

    y_train = y_train_all[fil_train]
    y_test = y_test_all[fil_test]

    x_train = x_train_all[fil_train]
    x_test = x_test_all[fil_test]

    return (x_train, y_train), (x_test, y_test)

def plot_sample(imgs, labels, nrows, ncols, resize=None, tograyscale=False):
    # create a grid of images
    fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))
    # take a random sample of images
    indices = np.random.choice(len(imgs), size=nrows*ncols, replace=False)
    for ax, idx in zip(axs.reshape(-1), indices):
        ax.axis('off')
        # sample an image
        ax.set_title(labels[idx])
        im = imgs[idx]
        if isinstance(im, np.ndarray):
            im = Image.fromarray(im)  
        if resize is not None:
            im = im.resize(resize)
        if tograyscale:
            im = im.convert('L')
        ax.imshow(im, cmap='gray')

def save_keras_model(model, filename):
    """
    Saves a Keras model to disk.
    Example of usage:

    >>> model = Sequential()
    >>> model.add(Dense(...))
    >>> model.compile(...)
    >>> model.fit(...)
    >>> save_keras_model(model, 'my_model.h5')

    :param model: the model to save;
    :param filename: string, path to the file in which to store the model.
    :return: the model.
    """
    save_model(model, filename)

def plot_history(history):
    plt.figure(figsize=(15, 5))
    plt.subplot(121)

    # Auxiliary info and funcs
    best_epoch = np.argmax(history.history['val_accuracy'])
    epochs = history.epoch
    smooth = lambda y: np.polyval(np.polyfit(epochs, y, deg=5), epochs)

    # Plot training accuracy values
    plt.axvline(best_epoch, label='best_epoch', c='k', ls='--', alpha=0.3)
    # Empirical values
    plt.plot(history.history['accuracy'], label='train_accuracy', c='C0')
    plt.ylabel('training accuracy')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(122)
    # Plot validation accuracy values
    plt.axvline(best_epoch, label='best_epoch', c='k', ls='--', alpha=0.3)
    # Empirical values
    plt.plot(history.history['val_accuracy'], label='val_accuracy', c='C1')
    plt.ylabel('validation accuracy')
    plt.xlabel('epoch')
    plt.legend()

if __name__ == '__main__':

    # Load the test CIFAR-10 data
    (x_train, y_train), (x_test, y_test) = load_cifar10(num_classes=3)
    #print(x_train.shape)
    #print(y_train.shape)
    #print(x_test.shape)
    #print(y_test.shape)
    #plot_sample(x_train, y_train, 4, 4)


    # use 20% of the training data as validation set
    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=4)

    #print('Training, validation, test samples: {}, {}, {}'.format(len(x_train), len(x_val), len(x_test)))
    #print('Training, validation, test samples: {}, {}, {}'.format(len(y_train), len(y_val), len(y_test)))

    # Normalize to 0-1 range
    #print(x_train)
    x_train = x_train.astype('float32') / 255.
    x_val = x_val.astype('float32') / 255.
    x_test = x_test.astype('float32') / 255.
    #print(x_train.shape)
    #print(x_val.shape)
    #print(x_test.shape)

    # one-hot encoding of the labels
    n_classes = 3
    y_train = utils.to_categorical(y_train, n_classes)
    y_val = utils.to_categorical(y_val, n_classes)
    y_test = utils.to_categorical(y_test, n_classes)
    #print(y_train.shape)
    #print(y_val.shape)
    #print(y_test.shape)


    #Flatten the images into 1D vectors
    #print(x_train.shape)
    x_train_reshaped = tf.reshape(x_train, [-1, 32 * 32 * 3])
    #print(x_train_reshaped.shape)

    #print(x_val.shape)
    x_val_reshaped = tf.reshape(x_val, [-1, 32 * 32 * 3])
    #print(x_val_reshaped.shape)

    #print(x_test.shape)
    x_test_reshaped = tf.reshape(x_test, [-1, 32 * 32 * 3])
    #print(x_test_reshaped.shape)

    #y_train_reshaped = tf.reshape(y_train, [12000])
    #print(y_train.shape)
    #print(y_train)
    #print(y_train_reshaped.shape)
    #print(y_train_reshaped)

    #y_val_reshaped = tf.reshape(y_val, [-1])
    #print(y_val.shape)
    #print(y_val) 
    #print(y_val_reshaped.shape)
    #print(y_val_reshaped)

    #y_test_reshaped = tf.reshape(y_test, [-1])
    #print(y_test.shape)
    #print(y_test)
    #print(y_test_reshaped.shape)
    #print(y_test_reshaped)


    # Build a Feed Forward Neural Network of your choice, following these constraints
    # Define the network
    network = Sequential()
    # Use only Dense layers
    # Use no more than 3 layers, considering also the output one
    # Use ReLU activation for all layers other than the output one
    #print(x_train_reshaped.shape)
    network.add(Dense(32, activation='relu', input_shape=(x_train_reshaped.shape[1:])))
    # Use Softmax activation for the output layer
    network.add(Dense(3, activation='softmax'))


    # Train the model on the training set from point 1 for 500 epochs
    # Use the RMSprop optimization algorithm, with a learning rate of 0.003 and a batch size of 128
    # Use categorical cross-entropy as a loss function
    network.compile(optimizer=optimizers.RMSprop(learning_rate=0.003), metrics=['accuracy'], loss='categorical_crossentropy')
    #network.summary()


    # Train the model on the training set from point 1 for 500 epochs
    # Implement early stopping, monitoring the validation accuracy of the model with a patience of 10 epochs and use 20% of the training data as validation set

    batch_size = 128

    es = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)

    history = network.fit(x_train_reshaped, y_train, batch_size=batch_size, epochs=500, validation_data=(x_val_reshaped, y_val), verbose=2, callbacks=[es])

    scores = network.evaluate(x_test_reshaped, y_test)

    print('Test loss: {} - Accuracy: {}'.format(*scores))

    # When early stopping kicks in, and the training procedure stops, restore the best model found during training.

    plot_history(history)

    #from google.colab import drive
    #drive.mount('/content/drive')

    #print(y_train)
    #MODEL_PATH = './drive/My Drive/Colab/USI/machine_learning/assignments/2/nn_task2.h5'
    MODEL_PATH = './nn_task2.h5'

    # Now save model in drive
    save_keras_model(network, MODEL_PATH)