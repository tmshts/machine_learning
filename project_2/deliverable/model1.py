# -*- coding: utf-8 -*-
"""model1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fkuGyV-yrNqp1J66WYE2FGPvXB_st0wp
"""

import os
import urllib.request as http
from zipfile import ZipFile
import tensorflow as tf
import numpy as np
from PIL import Image
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import save_model, load_model
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras import utils
from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, Dense
from tensorflow.keras import Sequential
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping


def load_cifar10(num_classes=3):
    """
    Downloads CIFAR-10 dataset, which already contains a training and test set,
    and return the first `num_classes` classes.
    Example of usage:

    >>> (x_train, y_train), (x_test, y_test) = load_cifar10()

    :param num_classes: int, default is 3 as required by the assignment.
    :return: the filtered data.
    """
    (x_train_all, y_train_all), (x_test_all, y_test_all) = cifar10.load_data()

    fil_train = tf.where(y_train_all[:, 0] < num_classes)[:, 0]
    fil_test = tf.where(y_test_all[:, 0] < num_classes)[:, 0]

    y_train = y_train_all[fil_train]
    y_test = y_test_all[fil_test]

    x_train = x_train_all[fil_train]
    x_test = x_test_all[fil_test]

    return (x_train, y_train), (x_test, y_test)

def plot_sample(imgs, labels, nrows, ncols, resize=None, tograyscale=False):
    # create a grid of images
    fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))
    # take a random sample of images
    indices = np.random.choice(len(imgs), size=nrows*ncols, replace=False)
    for ax, idx in zip(axs.reshape(-1), indices):
        ax.axis('off')
        # sample an image
        ax.set_title(labels[idx])
        im = imgs[idx]
        if isinstance(im, np.ndarray):
            im = Image.fromarray(im)  
        if resize is not None:
            im = im.resize(resize)
        if tograyscale:
            im = im.convert('L')
        ax.imshow(im, cmap='gray')

def save_keras_model(model, filename):
    """
    Saves a Keras model to disk.
    Example of usage:

    >>> model = Sequential()
    >>> model.add(Dense(...))
    >>> model.compile(...)
    >>> model.fit(...)
    >>> save_keras_model(model, 'my_model.h5')

    :param model: the model to save;
    :param filename: string, path to the file in which to store the model.
    :return: the model.
    """
    save_model(model, filename)

def plot_history(history):
    plt.figure(figsize=(15, 5))
    plt.subplot(121)

    # Auxiliary info and funcs
    best_epoch = np.argmax(history.history['val_accuracy'])
    epochs = history.epoch
    smooth = lambda y: np.polyval(np.polyfit(epochs, y, deg=5), epochs)

    # Plot training accuracy values
    plt.axvline(best_epoch, label='best_epoch', c='k', ls='--', alpha=0.3)
    # Empirical values
    plt.plot(history.history['accuracy'], label='train_accuracy', c='C0')
    plt.ylabel('training accuracy')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(122)
    # Plot validation accuracy values
    plt.axvline(best_epoch, label='best_epoch', c='k', ls='--', alpha=0.3)
    # Empirical values
    plt.plot(history.history['val_accuracy'], label='val_accuracy', c='C1')
    plt.ylabel('validation accuracy')
    plt.xlabel('epoch')
    plt.legend()

if __name__ == '__main__':

    # Load the test CIFAR-10 data
    (x_train, y_train), (x_test, y_test) = load_cifar10(num_classes=3)
    #print(x_train.shape)
    #print(y_train.shape)
    #print(x_test.shape)
    #print(y_test.shape)
    #plot_sample(x_train, y_train, 4, 4)


    # use 20% of the training data as validation set
    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=4)

    #print('Training, validation, test samples: {}, {}, {}'.format(len(x_train), len(x_val), len(x_test)))
    #print('Training, validation, test samples: {}, {}, {}'.format(len(y_train), len(y_val), len(y_test)))

    # Normalize to 0-1 range
    #print(x_train)
    x_train = x_train.astype('float32') / 255.
    x_val = x_val.astype('float32') / 255.
    x_test = x_test.astype('float32') / 255.
    #print(x_train.shape)
    #print(x_val.shape)
    #print(x_test.shape)

    # one-hot encoding of the labels
    n_classes = 3
    y_train = utils.to_categorical(y_train, n_classes)
    y_val = utils.to_categorical(y_val, n_classes)
    y_test = utils.to_categorical(y_test, n_classes)
    #print(y_train.shape)
    #print(y_val.shape)
    #print(y_test.shape)

    # Build a neural network with the following architecture
    # Convolutional layer, with 8 filters of size 5 by 5, stride of 1 by 1, and ReLU activation
    model = Sequential()
    model.add(Conv2D(8, (5, 5), strides=(1, 1), activation='relu', input_shape=(32, 32, 3)))

    # Max pooling layer, with pooling size of 2 by 2;
    model.add(MaxPooling2D(pool_size=(2, 2)))

    #Convolutional layer, with 16 filters of size 3 by 3, stride of 2 by 2, and ReLU activation;
    model.add(Conv2D(16, (3, 3), strides=(2, 2), activation='relu'))

    #Average pooling layer, with pooling size of 2 by 2;
    model.add(AveragePooling2D(pool_size=(2, 2)))

    #Layer to convert the 2D feature maps to vectors (Flatten layer);
    model.add(Flatten())

    #Dense layer with 8 neurons and tanh activation;
    model.add(Dense(8, activation='tanh'))

    #Dense output layer with softmax activation;
    model.add(Dense(3, activation='softmax'))

    # Use the RMSprop optimization algorithm, with a learning rate of 0.003 and a batch size of 128
    # Use categorical cross-entropy as a loss function
    model.compile(optimizer=optimizers.RMSprop(learning_rate=0.003), metrics=['accuracy'], loss='categorical_crossentropy')
    #model.summary()

    # Train the model on the training set from point 1 for 500 epochs

    # Implement early stopping, monitoring the validation accuracy of the model with a patience of 10 epochs and use 20% of the training data as validation set

    batch_size = 128

    es = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)

    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=500, validation_data=(x_val, y_val), verbose=1, callbacks=[es])

    scores = model.evaluate(x_test, y_test)

    print('Test loss: {} - Accuracy: {}'.format(*scores))

    # When early stopping kicks in, and the training procedure stops, restore the best model found during training.


    plot_history(history)

    #from google.colab import drive
    #drive.mount('/content/drive')

    #print(y_train)
    #MODEL_PATH = './drive/My Drive/Colab/USI/machine_learning/assignments/2/nn_task1.h5'
    MODEL_PATH = './nn_task1.h5'

    # Now save model in drive
    save_keras_model(model, MODEL_PATH)